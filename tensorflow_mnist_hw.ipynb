{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0Yf4NBJUSNM"
   },
   "source": [
    "# Создание нейронной сети\n",
    "\n",
    "В этом задании мы создадим полносвязную нейронную сеть используя при этом низкоуровневые механизмы tensorflow.\n",
    "\n",
    "Архитектутра нейросети представлена на следующем рисунке. Как видите, в ней имеется один входной слой, два скрытых, а так же выходной слой. В качестве активационной функции в скрытых слоях будет использоваться сигмоида. На выходном слое мы используем softmax.\n",
    "\n",
    "Часть кода по созданию сети уже написана, от вас требуется заполнить пропуски в указанных местах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01rZWUu0USNQ"
   },
   "source": [
    "## Архитектура нейронной сети\n",
    "\n",
    "<img src=\"http://cs231n.github.io/assets/nn1/neural_net2.jpeg\" alt=\"nn\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLvIZ705Qw_V"
   },
   "source": [
    "## О датасете MNIST\n",
    "\n",
    "Данную нейросеть мы будем обучать на датасете MNIST. Этот датасет представляет собой большое количество изображений рукописных цифр размером $28 \\times 28$ пикселей. Каждый пиксель принимает значение от 0 до 255.\n",
    "\n",
    "Как и раньше датасет будет разеделен на обучающую и тестовую выборки. При этом мы выполним нормализацию всех изображений, чтобы значения пикселей находились в промежутке от 0 до 1, разделив яркость каждого пикселя на 255.\n",
    "\n",
    "Кроме того, архитектура нейронной сети ожидает на вход вектор. В нашем же случае каждый объект выборки представляет собой матрицу. Что же делать? В этом задании мы \"растянем\" матрицу $28 \\times 28$, получив при этом вектор, состоящей из 784 элементов.\n",
    "\n",
    "![MNIST Dataset](https://www.researchgate.net/profile/Steven-Young-5/publication/306056875/figure/fig1/AS:393921575309346@1470929630835/Example-images-from-the-MNIST-dataset.png)\n",
    "\n",
    "Больше информации о датасете можно найти [здесь](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "il_0_5OyUSNR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cd-1_abTUSNS"
   },
   "outputs": [],
   "source": [
    "num_classes = 10  # загальна кількість класів, у нашому випадку це цифри від 0 до 9\n",
    "num_features = 784  # кількість атрибутів вхідного вектора 28 * 28 = 784\n",
    "\n",
    "learning_rate = 0.001  # швидкість навчання нейронної мережі\n",
    "training_steps = 3000  # максимальна кількість епох\n",
    "batch_size = 256  # перераховувати ваги мережі ми будемо не на усій вибірці, а на її випадковій підмножині розміром batch_size\n",
    "display_step = 100  # кожні 100 ітерацій ми будемо показувати поточне значення функції втрат та точності\n",
    "\n",
    "n_hidden_1 = 128  # кількість нейронів у 1-ому шарі\n",
    "n_hidden_2 = 256  # кількість нейронів у 2-ому шарі\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pGTXiRyTUSNT"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Завантажуємо датасет\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Перетворюємо цілі числові пікселі у тип float32\n",
    "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
    "\n",
    "# Перетворюємо матриці розміром 28x28 пікселів у вектор з 784 елементів\n",
    "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
    "\n",
    "# Нормалізуємо значення пікселів\n",
    "x_train, x_test = x_train / 255., x_test / 255.\n",
    "\n",
    "# Перемішуємо тренувальні дані\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FkRmCQjnUSNV"
   },
   "outputs": [],
   "source": [
    "class DenseLayer(tf.Module):\n",
    "    def __init__(self, in_features, out_features, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.weights = tf.Variable(tf.random.normal([in_features, out_features]))\n",
    "        self.bias = tf.Variable(tf.zeros([out_features]))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        linear = tf.matmul(x, self.weights) + self.bias\n",
    "        return tf.nn.relu(linear)\n",
    "\n",
    "class NN(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        # Первий прихований шар, що складається з 128 нейронів\n",
    "        self.layer1 = DenseLayer(num_features, n_hidden_1, name=\"Layer1\")\n",
    "\n",
    "        # Другий прихований шар, що складається з 256 нейронів\n",
    "        self.layer2 = DenseLayer(n_hidden_1, n_hidden_2, name=\"Layer2\")\n",
    "\n",
    "        # Вихідний шар\n",
    "        self.out_layer = DenseLayer(n_hidden_2, num_classes, name=\"OutLayer\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Подача вхідного вектора через перший прихований шар\n",
    "        layer1_out = self.layer1(x)\n",
    "\n",
    "        # Подача виходу першого прихованого шару в другий прихований шар\n",
    "        layer2_out = self.layer2(layer1_out)\n",
    "\n",
    "        # Подача виходу другого прихованого шару в вихідний шар\n",
    "        out_layer_out = self.out_layer(layer2_out)\n",
    "\n",
    "        # Зверніть увагу, що функцію softmax ми використовуємо для того, щоб вивід\n",
    "        # нейронної мережі мав значення від 0 до 1, відповідно ймовірності належності вхідного\n",
    "        # об'єкта до одного з 10 класів\n",
    "\n",
    "        x = tf.nn.softmax(out_layer_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LIf3o7VAUSNV"
   },
   "outputs": [],
   "source": [
    "# Функція для обчислення кросс-ентропії\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Кодуємо мітку у вигляді one-hot вектора.\n",
    "    y_true = tf.one_hot(y_true, depth=num_classes)\n",
    "\n",
    "    # Обмежуємо значення передбачення, щоб уникнути помилки log(0).\n",
    "    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "\n",
    "    # Обчислюємо кросс-ентропію\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n",
    "\n",
    "# Функція для обчислення точності\n",
    "def accuracy(y_pred, y_true):\n",
    "    # Порівнюємо передбачені класи зі справжніми класами\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "    \n",
    "    # Обчислюємо середню точність в наборі даних\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MQeT1yatUSNW"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Виклик функції навчання зі зазначенням кількості ітерацій\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m train(neural_net, train_data, num_epochs\u001b[38;5;241m=\u001b[39mtraining_steps)\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(nn, dataset, num_epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Включаємо автоматичне диференціювання\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m g:\n\u001b[1;32m---> 13\u001b[0m         pred \u001b[38;5;241m=\u001b[39m nn(batch_x)\n\u001b[0;32m     14\u001b[0m         loss \u001b[38;5;241m=\u001b[39m cross_entropy(pred, batch_y)\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;66;03m# Створюємо список оптимізованих параметрів\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m, in \u001b[0;36mNN.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m layer2_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(layer1_out)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Подача виходу другого прихованого шару в вихідний шар\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m out_layer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer(layer2_out)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Зверніть увагу, що функцію softmax ми використовуємо для того, щоб вивід\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# нейронної мережі мав значення від 0 до 1, відповідно ймовірності належності вхідного\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# об'єкта до одного з 10 класів\u001b[39;00m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(out_layer_out)\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mDenseLayer.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 8\u001b[0m     linear \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(linear)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1466\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1462\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1464\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1465\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[1;32m-> 1466\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m func(x, y, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1468\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1469\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1837\u001b[0m, in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1835\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops\u001b[38;5;241m.\u001b[39madd(x, y, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1837\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops\u001b[38;5;241m.\u001b[39madd_v2(x, y, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:491\u001b[0m, in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    490\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m    492\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAddV2\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, x, y)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    494\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Створюємо екземпляр нейронної мережі\n",
    "neural_net = NN(name=\"mnist\")\n",
    "\n",
    "# Функція навчання нейромережі\n",
    "def train(nn, dataset, num_epochs):\n",
    "    # Для підгонки ваг використовуємо стохастичний градієнтний спуск\n",
    "    optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_x, batch_y in dataset:\n",
    "            # Включаємо автоматичне диференціювання\n",
    "            with tf.GradientTape() as g:\n",
    "                pred = nn(batch_x)\n",
    "                loss = cross_entropy(pred, batch_y)\n",
    "\n",
    "                # Створюємо список оптимізованих параметрів\n",
    "                trainable_variables = nn.trainable_variables\n",
    "\n",
    "                # Обчислюємо градієнт відносно параметрів\n",
    "                gradients = g.gradient(loss, trainable_variables)\n",
    "\n",
    "                # Модифікуємо параметри відповідно до градієнту\n",
    "                optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        if (epoch + 1) % display_step == 0:\n",
    "            pred = nn(x_train)\n",
    "            loss = cross_entropy(pred, y_train)\n",
    "            acc = accuracy(pred, y_train)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss}, Accuracy: {acc}\")\n",
    "\n",
    "# Виклик функції навчання зі зазначенням кількості ітерацій\n",
    "train(neural_net, train_data, num_epochs=training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnyns9lBfpQZ"
   },
   "outputs": [],
   "source": [
    "# Тренування мережі\n",
    "\n",
    "loss_history = []  # кожні display_step кроків зберігайте в цьому списку поточну помилку нейромережі\n",
    "accuracy_history = [] # кожні display_step кроків зберігайте в цьому списку поточну точність нейромережі\n",
    "\n",
    "# У цьому циклі ми будемо проводити навчання нейронної мережі\n",
    "# з тренувального датасету train_data вийміть випадкову підмножину, на якій\n",
    "# відбудеться тренування. Використовуйте метод take, доступний для тренувального датасету.\n",
    "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps)):\n",
    "    # Оновлюємо ваги нейронної мережі\n",
    "    train(neural_net, batch_x, batch_y)\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = neural_net(batch_x)\n",
    "        current_loss = cross_entropy(pred, batch_y)\n",
    "        loss_history.append(current_loss)\n",
    "        \n",
    "        current_accuracy = accuracy(pred, batch_y)\n",
    "        accuracy_history.append(current_accuracy)\n",
    "        print(f\"Step: {step}, Loss: {current_loss}, Accuracy: {current_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yCBfG6MbQB2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Виведення графіків\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Графік залежності втрат від кроку навчання\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(display_step, training_steps + 1, display_step), loss_history, marker='o')\n",
    "plt.title('Зміна втрат від кроку навчання')\n",
    "plt.xlabel('Крок навчання')\n",
    "plt.ylabel('Втрати')\n",
    "\n",
    "# Графік залежності точності від кроку навчання\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(display_step, training_steps + 1, display_step), accuracy_history, marker='o', color='orange')\n",
    "plt.title('Зміна точності від кроку навчання')\n",
    "plt.xlabel('Крок навчання')\n",
    "plt.ylabel('Точність')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LE3g4gDyUSNY"
   },
   "outputs": [],
   "source": [
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.batch(batch_size)\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Пройдемося по тестовим даним\n",
    "for batch_x, batch_y in test_data:\n",
    "    pred = neural_net(batch_x)\n",
    "    batch_y = tf.cast(batch_y, tf.int64)  # Перетворюємо тип даних batch_y на int64\n",
    "    correct = tf.equal(tf.argmax(pred, 1), batch_y)\n",
    "    total_correct += tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    total_samples += batch_x.shape[0]\n",
    "\n",
    "# Обчислюємо точність на тестових даних\n",
    "test_accuracy = total_correct / total_samples\n",
    "print(\"Точність на тестових даних: {:.4f}\".format(test_accuracy.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EEHAubOUSNY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Випадково виберемо 5 індексів з тестових даних\n",
    "random_indices = random.sample(range(len(x_test)), 5)\n",
    "\n",
    "# Збережемо вибрані зображення та їх класи\n",
    "selected_images = x_test[random_indices]\n",
    "selected_labels = y_test[random_indices]\n",
    "\n",
    "# Прогнозуємо класи для вибраних зображень\n",
    "predictions = neural_net(selected_images)\n",
    "predicted_classes = tf.argmax(predictions, axis=1)\n",
    "\n",
    "# Візуалізуємо результати\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(selected_images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'Predicted: {predicted_classes[i].numpy()}, Actual: {selected_labels[i]}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tensorflow_mnist_hw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
